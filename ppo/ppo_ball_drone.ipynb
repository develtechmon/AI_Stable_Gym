{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class DroneChaseEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super(DroneChaseEnv, self).__init__()\n",
    "        \n",
    "        # Canvas size\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        \n",
    "        # Score\n",
    "        self.score = 0  # Tracks how many times the drone hits the balloon\n",
    "\n",
    "        # Drone parameters\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.max_speed = 5.0\n",
    "\n",
    "        # Balloon parameters\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), \n",
    "                                     np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), \n",
    "                                     np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "\n",
    "        # Action space: Continuous thrust adjustments for left and right propellers\n",
    "        # Thrust values between -1 and 1\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: Positions and velocities\n",
    "        # Drone position (x, y), Drone velocity (vx, vy), Balloon position (x, y)\n",
    "        low_obs = np.array([0, 0, -self.max_speed, -self.max_speed, 0, 0], dtype=np.float32)\n",
    "        high_obs = np.array([self.width, self.height, self.max_speed, self.max_speed, self.width, self.height], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        # Rendering\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == 'human':\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Episode parameters\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset drone position and velocity\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "        # Reset balloon position and velocity\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), \n",
    "                                     np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), \n",
    "                                     np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (array): Array of two thrust values [left_thrust, right_thrust], \n",
    "                            where each value is between -1 and 1.\n",
    "\n",
    "        Returns:\n",
    "            observation (array): Updated state containing [drone_position, drone_velocity, balloon_position].\n",
    "            reward (float): The reward for the current step.\n",
    "            done (bool): Whether the episode has ended (always False for continuous simulation).\n",
    "            truncated (bool): Whether the episode was truncated (always False here).\n",
    "            info (dict): Additional debugging information (empty here).\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Calculate the drone's thrust based on the action\n",
    "        # The left and right thrust values determine the horizontal and vertical motion of the drone.\n",
    "        left_thrust, right_thrust = action\n",
    "        thrust = np.array([right_thrust - left_thrust,  # Horizontal movement (right thrust - left thrust)\n",
    "                        (left_thrust + right_thrust) / 2],  # Vertical movement (average thrust)\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        # 2. Update the drone's velocity based on the thrust\n",
    "        self.drone_vel += thrust\n",
    "\n",
    "        # 3. Enforce the maximum speed limit\n",
    "        # If the drone's speed exceeds the allowed max_speed, scale it down to max_speed.\n",
    "        speed = np.linalg.norm(self.drone_vel)  # Calculate the magnitude of the velocity vector\n",
    "        if speed > self.max_speed:\n",
    "            self.drone_vel = (self.drone_vel / speed) * self.max_speed  # Scale the velocity vector\n",
    "\n",
    "        # 4. Update the drone's position based on the velocity\n",
    "        self.drone_pos += self.drone_vel\n",
    "\n",
    "        # 5. Ensure the drone stays within the canvas bounds\n",
    "        # If the position goes outside the defined environment, clip it to the boundaries.\n",
    "        self.drone_pos = np.clip(self.drone_pos, [0, 0], [self.width, self.height])\n",
    "\n",
    "        # 6. Calculate the distance between the drone and the balloon\n",
    "        # Use the Euclidean distance formula to determine how far the drone is from the balloon.\n",
    "        distance = np.linalg.norm(self.drone_pos - self.balloon_pos)\n",
    "\n",
    "        # 7. Check if the drone catches the balloon (within the catch radius)\n",
    "        catch_radius = 10.0  # Defines the radius within which the drone \"catches\" the balloon\n",
    "        if distance < catch_radius:\n",
    "            # If the drone catches the balloon:\n",
    "            # - Increment the score\n",
    "            self.score += 1\n",
    "            # - Respawn the balloon at a random location\n",
    "            self.balloon_pos = np.random.uniform([0, 0], [self.width, self.height])\n",
    "            # - Recalculate the distance to the new balloon position\n",
    "            distance = np.linalg.norm(self.drone_pos - self.balloon_pos)\n",
    "            # - Give a large reward for catching the balloon\n",
    "            reward = 10.0\n",
    "        else:\n",
    "            # If the balloon is not caught, penalize the drone based on the distance\n",
    "            # Closer to the balloon = less penalty; farther away = more penalty\n",
    "            reward = -distance * 0.01\n",
    "\n",
    "        # 8. Randomly move the balloon\n",
    "        # The balloon moves in small, random steps within the environment bounds.\n",
    "        self.balloon_pos += np.random.uniform(-2, 2, size=(2,))\n",
    "        self.balloon_pos = np.clip(self.balloon_pos, [0, 0], [self.width, self.height])  # Keep balloon in bounds\n",
    "\n",
    "        # 9. Penalize the drone for each time step to encourage efficiency\n",
    "        reward -= 0.1\n",
    "\n",
    "        # 10. Increment the step count\n",
    "        # Track how many steps have been taken in the episode.\n",
    "        self.current_step += 1\n",
    "\n",
    "        # 11. Create the observation array\n",
    "        # The observation includes the drone's position, velocity, and the balloon's position.\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        # 12. Return the updated state, reward, and additional info\n",
    "        # 'done' and 'truncated' are False here because this is a continuous simulation.\n",
    "        info = {}\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        return observation, reward, False, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != 'human':\n",
    "            return\n",
    "\n",
    "        import pygame\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "        self.screen.fill((255, 255, 255))  # White background\n",
    "\n",
    "        # Draw balloon\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), self.balloon_pos.astype(int), 10)  # Red circle\n",
    "\n",
    "        # Draw drone\n",
    "        pygame.draw.rect(self.screen, (0, 0, 255), (*self.drone_pos - 10, 20, 20))  # Blue square\n",
    "\n",
    "        # Add labels\n",
    "        font = pygame.font.Font(None, 24)  # Default font, size 24\n",
    "        balloon_label = font.render(\"Balloon\", True, (0, 0, 0))  # Black text for balloon\n",
    "        drone_label = font.render(\"Drone\", True, (0, 0, 0))  # Black text for drone\n",
    "\n",
    "        # Position labels near objects\n",
    "        self.screen.blit(balloon_label, (self.balloon_pos[0] - 20, self.balloon_pos[1] - 20))\n",
    "        self.screen.blit(drone_label, (self.drone_pos[0] - 20, self.drone_pos[1] - 20))\n",
    "        \n",
    "        # Display timer\n",
    "        time_remaining = max(0, 30 - self.current_step / 30)  # 30 FPS assumed\n",
    "        timer_label = font.render(f\"Time Remaining: {time_remaining:.1f}s\", True, (0, 0, 0))\n",
    "        self.screen.blit(timer_label, (10, 10))\n",
    "        \n",
    "        # Display the score\n",
    "        score_label = font.render(f\"Score: {self.score}\", True, (0, 0, 0))\n",
    "        self.screen.blit(score_label, (10, 40))  # Display below the timer\n",
    "        \n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(30)\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == 'human':\n",
    "            import pygame\n",
    "            pygame.quit()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate((self.drone_pos, self.drone_vel, self.balloon_pos)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Vectorize with periodic model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = \"models/PPO-ball-drone\"\n",
    "logdir = \"/home/jlukas/Desktop/My_Project/AI_Stable_GYM/logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "# Wrap with VecMonitor for logging rewards and episode lengths\n",
    "env = make_vec_env(lambda: DroneChaseEnv(), n_envs=4)\n",
    "env = VecMonitor(env, filename=None)  # Enables logging for TensorBoard\n",
    "\n",
    "# Define the PPO model with TensorBoard logging\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "# Train the agent with periodic model saving\n",
    "TIMESTEPS = 10000\n",
    "num_iterations = 10\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Starting iteration {i + 1}/{num_iterations}\")\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO-ball-drone\")\n",
    "    model.save(f\"{models_dir}/ppo_drone_chase_{TIMESTEPS * (i + 1)}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train without Vectorize and with periodic model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = \"models/PPO-ball-drone\"\n",
    "logdir = \"/home/jlukas/Desktop/My_Project/AI_Stable_GYM/logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "env = DroneChaseEnv()\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "\titers += 1\n",
    "\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "\tmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model directly without periodic model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = \"models/PPO-ball-drone\"\n",
    "logdir = \"/home/jlukas/Desktop/My_Project/AI_Stable_GYM/logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "env = DroneChaseEnv()\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=800000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_drone_chase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with vectorize without periodic model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to /home/jlukas/Desktop/My_Project/AI_Stable_GYM/logs/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlukas/Desktop/My_Project/AI_Stable_GYM/ai_gym/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2068 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1004         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051609054 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.00729      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 342          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.46e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006766282 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0141     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 289         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.16e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071708504 |\n",
      "|    clip_fraction        | 0.083        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.0119      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 65.4         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 387          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004186497 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0691      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 188         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 507         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 768         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006229207 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.0569      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 177         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 754          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049616825 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.22         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 154          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 236          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 746          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056892512 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.321        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 98.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 739         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008555088 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.7        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 733         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006858011 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 91.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 123        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00952906 |\n",
      "|    clip_fraction        | 0.098      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.56       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00791   |\n",
      "|    std                  | 0.964      |\n",
      "|    value_loss           | 66.1       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072882166 |\n",
      "|    clip_fraction        | 0.076        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.55         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.4         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00395     |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 723        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 147        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00868325 |\n",
      "|    clip_fraction        | 0.0962     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.72      |\n",
      "|    explained_variance   | 0.644      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.1       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00331   |\n",
      "|    std                  | 0.939      |\n",
      "|    value_loss           | 74.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 717          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076780864 |\n",
      "|    clip_fraction        | 0.0841       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.725        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 51.7         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    std                  | 0.94         |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011616946 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.8        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 93.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 712          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083662765 |\n",
      "|    clip_fraction        | 0.0838       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 113          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    std                  | 0.903        |\n",
      "|    value_loss           | 223          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 709         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007969713 |\n",
      "|    clip_fraction        | 0.0964      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.000217   |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 707         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012076514 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 94          |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 706         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011346858 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.7        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000481   |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 142         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013339899 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -3.22e-05   |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 79.9        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = \"models/PPO-ball-drone_v1\"\n",
    "logdir = \"/home/jlukas/Desktop/My_Project/AI_Stable_GYM/logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "# Wrap with VecMonitor for logging rewards and episode lengths\n",
    "env = make_vec_env(lambda: DroneChaseEnv(), n_envs=4)\n",
    "env = VecMonitor(env, filename=None)  # Enables logging for TensorBoard\n",
    "\n",
    "# Define the PPO model with TensorBoard logging\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_drone_chase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "\n",
    "models_dir = \"models/PPO-ball-drone_v1\"\n",
    "model_path = f\"{models_dir}/ppo_drone_chase\"\n",
    "\n",
    "# Create the environment for evaluation\n",
    "env = DroneChaseEnv(render_mode='human')\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(model_path)\n",
    "\n",
    "# Run the trained model in the environment\n",
    "observation, info = env.reset()\n",
    "episode_over = False\n",
    "\n",
    "while not episode_over:\n",
    "    # Use the trained model to predict actions\n",
    "    action, _ = model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
