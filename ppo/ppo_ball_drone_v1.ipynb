{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drone following ball my custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "\n",
    "class DroneBallEnvx(gym.Env):\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Canvas Size\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        \n",
    "        # Score\n",
    "        self.score = 0\n",
    "        \n",
    "        # Drone Parameters\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0, 0], dtype=np.float32)\n",
    "        self.max_speed = 5\n",
    "        \n",
    "        # Balloon Parameters\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "        \n",
    "        # Step counter\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 500  # Maximum steps per episode\n",
    "        \n",
    "        # Define action space\n",
    "        self.action_space = spaces.Discrete(4)  # 0: Up, 1: Down, 2: Left, 3: Right\n",
    "        \n",
    "        # Observation space: Drone position (x, y), Drone velocity (vx, vy), Balloon position (x, y)\n",
    "        low_obs = np.array([0, 0, -self.max_speed, -self.max_speed, 0, 0], dtype=np.float32)\n",
    "        high_obs = np.array([self.width, self.height, self.max_speed, self.max_speed, self.width, self.height], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        # Rendering\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == 'human':\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "            pygame.display.set_caption(\"Drone and Balloon Environment\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "            \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.score = 0\n",
    "        self.current_step = 0  # Reset step counter\n",
    "        \n",
    "        # Reset drone position and velocity\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0, 0], dtype=np.float32)\n",
    "        \n",
    "        # Reset balloon position and velocity\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "        \n",
    "        info = {}\n",
    "        observation = self.get_obs()\n",
    "        return observation, info\n",
    "        \n",
    "    def get_obs(self):\n",
    "        return np.concatenate((self.drone_pos, self.drone_vel, self.balloon_pos)).astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Increment step counter\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Update drone velocity based on discrete action\n",
    "        if action == 0:  # Up\n",
    "            self.drone_vel[1] -= 1\n",
    "        elif action == 1:  # Down\n",
    "            self.drone_vel[1] += 1\n",
    "        elif action == 2:  # Left\n",
    "            self.drone_vel[0] -= 1\n",
    "        elif action == 3:  # Right\n",
    "            self.drone_vel[0] += 1\n",
    "        \n",
    "        # Clip the velocity to max speed\n",
    "        self.drone_vel = np.clip(self.drone_vel, -self.max_speed, self.max_speed)\n",
    "        \n",
    "        # Update drone position\n",
    "        self.drone_pos += self.drone_vel\n",
    "        \n",
    "        # Ensure the drone stays within the canvas\n",
    "        self.drone_pos = np.clip(self.drone_pos, [0, 0], [self.width, self.height])\n",
    "        \n",
    "        # Update balloon position\n",
    "        self.balloon_pos += self.balloon_vel\n",
    "        \n",
    "        # Ensure the balloon stays within the canvas\n",
    "        self.balloon_pos = np.clip(self.balloon_pos, [0, 0], [self.width, self.height])\n",
    "        \n",
    "        # Calculate reward (example: negative distance to balloon)\n",
    "        distance_to_balloon = np.linalg.norm(self.drone_pos - self.balloon_pos)\n",
    "        reward = -distance_to_balloon\n",
    "        \n",
    "        # Check if terminated (example: if drone reaches balloon)\n",
    "        terminated = distance_to_balloon < 10 or self.current_step >= self.max_steps\n",
    "        \n",
    "        # Increment score\n",
    "        if distance_to_balloon < 40:\n",
    "            self.score += 1\n",
    "            print(self.score)\n",
    "            reward += 10  # Reward for catching the balloon\n",
    "            # Respawn balloon\n",
    "            self.balloon_pos = np.array([np.random.uniform(0, self.width), np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        \n",
    "        # Get observation\n",
    "        observation = self.get_obs()\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            # Handle Pygame events to prevent freezing\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    self.close()\n",
    "\n",
    "            # Clear the screen\n",
    "            self.screen.fill((255, 255, 255))  # Fill screen with white\n",
    "            \n",
    "            # Draw the drone\n",
    "            pygame.draw.circle(self.screen, (0, 0, 255), self.drone_pos.astype(int), 10)\n",
    "            \n",
    "            # Draw the balloon\n",
    "            pygame.draw.circle(self.screen, (255, 0, 0), self.balloon_pos.astype(int), 10)\n",
    "            \n",
    "            font = pygame.font.Font(None, 24)  # Default font, size 24\n",
    "            balloon_label = font.render(\"Balloon\", True, (0, 0, 0))  # Black text for balloon\n",
    "            drone_label = font.render(\"Drone\", True, (0, 0, 0))  # Black text for drone\n",
    "\n",
    "            # Position labels near objects\n",
    "            self.screen.blit(balloon_label, (self.balloon_pos[0] - 20, self.balloon_pos[1] - 20))\n",
    "            self.screen.blit(drone_label, (self.drone_pos[0] - 20, self.drone_pos[1] - 20))\n",
    "            \n",
    "            # Display the score and steps\n",
    "            font = pygame.font.Font(None, 24)\n",
    "            score_text = font.render(f\"Score: {self.score}\", True, (0, 0, 0))\n",
    "            step_text = font.render(f\"Step: {self.current_step}/{self.max_steps}\", True, (0, 0, 0))\n",
    "            self.screen.blit(score_text, (10, 10))\n",
    "            self.screen.blit(step_text, (10, 40))\n",
    "            \n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(30)\n",
    "    \n",
    "    def close(self):\n",
    "        if self.render_mode == 'human':\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test our environment without keyboard control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = DroneBallEnvx(render_mode='human')\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    for _ in range(500):  # Run for 500 steps\n",
    "        action = env.action_space.sample()  # Take random actions\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        \n",
    "        if terminated:\n",
    "            obs, info = env.reset()\n",
    "    \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test our environment with keyboard control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from ppo_drone_ball_env import DroneBallEnv  # Replace with your environment file path\n",
    "\n",
    "# Initialize environment\n",
    "env = DroneBallEnvx(render_mode='human')\n",
    "obs, info = env.reset()\n",
    "\n",
    "running = True\n",
    "while running:\n",
    "    # Process Pygame events\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        elif event.type == pygame.KEYDOWN:  # Only handle key presses\n",
    "            if event.key == pygame.K_UP:\n",
    "                action = int(0)  # Up\n",
    "            elif event.key == pygame.K_DOWN:\n",
    "                action = int(1)  # Down\n",
    "            elif event.key == pygame.K_LEFT:\n",
    "                action = int(2)  # Left\n",
    "            elif event.key == pygame.K_RIGHT:\n",
    "                action = int(3)  # Right\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Take a step in the environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Render the environment\n",
    "            env.render()\n",
    "\n",
    "            # Check if the episode is terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                print(f\"Episode ended. Score: {env.score}\")\n",
    "                obs, info = env.reset()\n",
    "\n",
    "    # Add a small delay for smoother rendering\n",
    "    pygame.time.delay(50)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To train our agent and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = r\"D:\\Reinforcement_Learning\\ppo\\model\"\n",
    "logdir = r\"D:\\Reinforcement_Learning\\ppo\\logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "# Wrap with VecMonitor for logging rewards and episode lengths\n",
    "env = make_vec_env(lambda: DroneBallEnvx(), n_envs=4)\n",
    "\n",
    "# Define the PPO model with TensorBoard logging\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "# Train the agent with periodic model saving\n",
    "TIMESTEPS = 100000\n",
    "num_iterations = 10\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Starting iteration {i + 1}/{num_iterations}\")\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO-ball-drone-deepseek\")\n",
    "    model.save(f\"{models_dir}/ppo_drone_chase_{TIMESTEPS * (i + 1)}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test and validate our model without episode stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = r\"D:\\Reinforcement_Learning\\ppo\\model\"\n",
    "logdir = r\"D:\\Reinforcement_Learning\\ppo\\logs\"\n",
    "\n",
    "model_path = f\"{models_dir}\\ppo_drone_chase_1000000\"\n",
    "\n",
    "# Create the environment for evaluation\n",
    "env = DroneBallEnvx(render_mode='human')\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(model_path)\n",
    "\n",
    "# Run the trained model in the environment\n",
    "observation, info = env.reset()\n",
    "episode_over = False\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while True:#not episode_over:\n",
    "    # Use the trained model to predict actions\n",
    "    action, _ = model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_over = terminated or truncated\n",
    "    \n",
    "    # Render the environment\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test and validate our model with episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "\n",
    "# Paths for saving models and logs\n",
    "models_dir = r\"D:\\Reinforcement_Learning\\ppo\\model\"\n",
    "logdir = r\"D:\\Reinforcement_Learning\\ppo\\logs\"\n",
    "\n",
    "model_path = f\"{models_dir}\\ppo_drone_chase_100000\"\n",
    "\n",
    "# Create the environment for evaluation\n",
    "env = DroneBallEnvx(render_mode='human')\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(model_path)\n",
    "\n",
    "# Run multiple episodes\n",
    "num_episodes = 5  # Number of episodes to run\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"Starting episode {episode + 1}/{num_episodes}\")\n",
    "    observation, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # Use the trained model to predict actions\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Episode {episode + 1} ended. Total reward: {reward}\")\n",
    "\n",
    "# Close the environment when all episodes are done\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
